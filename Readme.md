<p align="center">
  <img src="banner.png" alt="AIRA Banner" width="700"/>
</p>
<h1 align="center">ğŸ¤– AIRA â€“ AI Responsive Assistant</h1>
<p align="center">Your multilingual voice assistant with DeepSeek + LangChain + Streamlit</p>


**AIRA** is a fully customizable, multilingual AI-powered voice assistant built with cutting-edge technologies like `Streamlit`, `LangChain`, `Ollama`, and `Vosk`. It understands your voice commands, performs smart actions, and responds intelligently â€” all while maintaining conversation history and providing a pleasant, human-like interaction.

Whether you're looking to automate daily tasks, interact with an AI agent using natural voice input, or experiment with multilingual voice-to-AI pipelines, **AIRA** offers a powerful yet beginner-friendly platform to do it all.

---


## âœ¨ Key Features

| Feature                              | Description                                                                 |
|--------------------------------------|-----------------------------------------------------------------------------|
| ğŸ—£ï¸ Wake Word Detection               | Say "**Hey Aira**" (customizable) to activate the assistant hands-free     |
| ğŸ¤ Speech-to-Text                    | Uses **Vosk** for accurate offline voice transcription                     |
| ğŸŒ Multilingual Support              | Accepts voice input in **English, Hindi, Kannada, Malayalam, Telugu, Tamil** with auto-translation to English |
| ğŸ¤– LLM Integration                   | Uses **Ollama + DeepSeek-R1:8B model** via LangChain for intelligent responses |
| ğŸ”„ Conversation Memory               | Tracks dialogue context using LangChain's `ChatMessageHistory`             |
| ğŸ”Š Text-to-Speech                    | Speaks responses aloud using `pyttsx3`, with adjustable speed              |
| ğŸ§  Smart System Actions              | Executes commands like "open YouTube", "what's the time?", "shutdown", etc. |
| âœï¸ Manual Query Support              | Allows typed input in addition to voice input                              |
| ğŸ“ Chat History Export               | Saves entire conversation with timestamp to a `.txt` file                  |
| ğŸ§© Modular, Extensible Codebase      | Easy to plug in new models, UI customizations, or extra voice features     |

---

## ğŸ“‚ Project Structure

AIRA/
â”œâ”€â”€ models/
â”‚ â””â”€â”€ vosk-model-small-en-us-0.15/ # Required for offline speech recognition and can change according to requirement
â”œâ”€â”€ main.py # Main Streamlit app with AI assistant logic
â”œâ”€â”€ requirements.txt # Dependency list
â””â”€â”€ README.md # Project documentation


---

![AIRA Banner](UI.png)

---


## ğŸ§  How It Works â€” `main.py` Explained

### ğŸ”§ Initialization & Setup

- Loads **Streamlit UI** and sidebar for configuration (TTS speed, language, wake word, etc.)
- Initializes **text-to-speech (TTS)** engine using `pyttsx3`
- Loads **Vosk offline speech recognition model** from `/models`
- Sets up **LangChain with Ollama** using the `deepseek-r1:8b` model (streaming mode)
- Maintains **chat history** across queries using `ChatMessageHistory`

---

### ğŸ—£ï¸ Voice Interaction Flow

1. **Wake Word Detection**  
   Listens in the background for a user-defined trigger (default: `"hey aira"`). Once detected, it records your voice.

2. **Speech Transcription & Translation**  
   Uses **Vosk** to transcribe speech. If a non-English language is selected, it uses `deep_translator` to translate to English (before sending to the AI model).

3. **AI Response Generation**  
   The input (voice or text) is passed into a **prompt template** that includes conversation history. `LangChain` formats the prompt and sends it to **DeepSeek-R1:8B** via **Ollama**, and returns a streamed response.

4. **Response Handling**  
   The AIâ€™s answer is:
   - Spoken aloud using `pyttsx3`
   - Displayed on the UI
   - Stored in the sessionâ€™s history

---

### ğŸ”„ Action Handler

Handles system-level commands like:
- `"open YouTube"`, `"open Gmail"`, `"search for cats"`
- `"shutdown"`, `"restart"`, `"what time is it?"`, `"open calculator"`
- You can expand this easily in the `handle_actions()` function

---

### âœï¸ Manual Input Support

In case your mic doesn't work or you prefer typing, AIRA provides:
- A text box to manually enter a query
- A "Submit Query" button that processes it like a voice command

---

### ğŸ’¬ Chat Management

- Each message is stored using `ChatMessageHistory` (type: user/AI)
- On-screen chat history with emojis
- "Export Chat" button saves conversation to a `.txt` file with a timestamp

---

## âš™ï¸ Installation & Setup

### ğŸ”— 1. Clone the Repo

```bash
git clone https://github.com/<your-username>/aira.git
cd aira
```

---

### ğŸ“¦ 2. Install Dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ’¡ 3. Set Up Vosk Model
Download from: https://alphacephei.com/vosk/models

Place the model folder like this:

```bash
AIRA/models/vosk-model-small-en-us-0.15/
```

---

### ğŸ¤– 4. Pull Ollama Model
Ensure you have Ollama installed. Then run:

```bash
ollama pull deepseek-r1:8b
```

---

### ğŸ 5. Run the Assistant

```bash
streamlit run main.py
```

---

ğŸŒ Language Support
Your voice input can be in:

Language	Code
English	    en
Hindi	    hi
Kannada	    kn
Malayalam	ml
Telugu	    te
Tamil	    ta

Transcription is translated to English (if enabled) for best LLM response quality.

---

### ğŸ” Dependencies
Hereâ€™s a sample requirements.txt:
```bash
~streamlit
~sounddevice
~vosk
~pyttsx3
~numpy
~deep-translator
~langchain
~langchain-community
~langchain-core
~langchain-ollama
```
You can generate exact versions with pip freeze > requirements.txt.

---

### ğŸ§© Extending the Assistant

You can enhance AIRA by:

=> Adding custom actions like controlling IoT devices

=> Integrating camera/image recognition

=> Switching models (e.g., LLaMA 3, Phi-3) using Ollama

=> Using OpenAI API instead of Ollama

=> Deploying on a server with wake-word listener running in background

=> Using a higher upgrade vsok model for a greater accuracy

---

### ğŸ‘¨â€ğŸ’» Author
Sumukh Mallikarjuna
Passionate about AI, LLMs, and voice-based interaction systems.
ğŸ“Œ GitHub: @Sumukh-15

---

### ğŸ“„ License

MIT License

Copyright (c) 2025 Sumukh

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the â€œSoftwareâ€), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...

[Truncated for brevity. You can copy full MIT license from: https://opensource.org/license/mit]

---

### ğŸ’¬ Example Use Cases

"Hey Aira, open YouTube" â†’ Launches browser with YouTube

"What time is it?" â†’ Responds with current system time

"Tell me a fact about black holes" â†’ LLM response

"Search for Python tutorials" â†’ Opens Google search

Type: "What is the capital of Japan?" â†’ AI replies: Tokyo

---

### ğŸ”Š AIRA is your multilingual, intelligent, offline-friendly AI assistant â€” simple, powerful, and private.

---

